#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
RLHF í†µí•© ê±´ì¶• ì„¤ê³„ ìµœì í™” ì‹œìŠ¤í…œ

ê¸°ì¡´ rl_architecture_optimizer.pyë¥¼ ë² ì´ìŠ¤ë¡œ í•˜ì—¬ RLHF ê¸°ëŠ¥ì„ í†µí•©í•œ ë²„ì „
- ì¸ê°„ í”¼ë“œë°± ê¸°ë°˜ ë³´ìƒ ëª¨ë¸ í†µí•©
- 3ê°€ì§€ ê°€ì¤‘ì¹˜ ì¡°í•© ì§€ì› (0.3, 0.5, 0.7)
- ì—°ì† í•™ìŠµ (ê° ê°€ì¤‘ì¹˜ë‹¹ 3ë¼ìš´ë“œ)
- Closed Brep ì²˜ë¦¬ ë¡œì§ ìœ ì§€
- ìë™í™”ëœ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±
"""

import os
import sys
import time
import json
import argparse
import threading
import queue
import datetime
import signal
import numpy as np
import pandas as pd
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
import zmq
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Union, Optional, Any
from pathlib import Path
import matplotlib.pyplot as plt
from collections import Counter

# === ê²½ë¡œ ì„¤ì • ===
BASE_DIR = Path(r"C:\Users\valen\Desktop\Dev\6. RLHF")
MODULES_DIR = BASE_DIR / "python_modules"
DATA_DIR = BASE_DIR / "data"
ZMQ_LOGS_DIR = DATA_DIR / "zmq_logs"

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(MODULES_DIR))

# === íŒŒì¼ ê²½ë¡œ ì„¤ì • ===
DEFAULT_PATHS = {
    'reward_model': BASE_DIR / "python_modules" / "improved_reward_models" / "improved_reward_model_symmetry_20250530_165846.pt",
    'initial_ppo_model': BASE_DIR / "data" / "models" / "ppo_architecture_20250523_162526" / "final_model.zip",
    'base_output_dir': BASE_DIR / "rlhf_experiments"
}

# === ê¸€ë¡œë²Œ ë³€ìˆ˜ ===
STATE_QUEUE = queue.Queue()
STOP_EVENT = threading.Event()
LAST_STATE = None
DEBUG = False

# === ë¡œê¹… í•¨ìˆ˜ë“¤ (ê¸°ì¡´ RLê³¼ ë™ì¼) ===
def log_info(message):
    print(message)

def log_warning(message):
    print(f"\033[93mâš ï¸ {message}\033[0m")

def log_error(message):
    print(f"\033[91mâŒ {message}\033[0m")

def log_success(message):
    print(f"\033[92mâœ… {message}\033[0m")

def log_debug(message):
    if DEBUG:
        print(f"\033[94mğŸ” {message}\033[0m")

# === ì‹ í˜¸ í•¸ë“¤ëŸ¬ ===
def signal_handler(sig, frame):
    log_info("\nğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    STOP_EVENT.set()

# === ëª¨ë“ˆ ì„í¬íŠ¸ ===
try:
    from reward_adapter import create_reward_function
    REWARD_ADAPTER_AVAILABLE = True
except ImportError as e:
    REWARD_ADAPTER_AVAILABLE = False
    log_warning(f"Reward adapter import failed: {e}")

# ì¸ê°„ í”¼ë“œë°± ë³´ìƒ ëª¨ë¸ ê´€ë ¨
try:
    from architectural_reward_model import ProbabilisticArchitecturalModel_V2
    HUMAN_REWARD_MODEL_AVAILABLE = True
    
    # ğŸ” ì§„ë‹¨ ì½”ë“œ ì¶”ê°€
    print(f"ğŸ” ì„í¬íŠ¸ ê²°ê³¼:")
    print(f"   ProbabilisticArchitecturalModel_V2 = {ProbabilisticArchitecturalModel_V2}")
    print(f"   type = {type(ProbabilisticArchitecturalModel_V2)}")
    print(f"   callable = {callable(ProbabilisticArchitecturalModel_V2)}")
    
except ImportError as e:
    HUMAN_REWARD_MODEL_AVAILABLE = False
    print(f"âŒ Human reward model import failed: {e}")

# === ê¸°ì¡´ imports ë‹¤ìŒì— ì¶”ê°€ ===
def normalize_rewards_for_rlhf(env_reward, human_reward):
    """ê°€ì¤‘ì¹˜ë³„ ë¹„êµ ì‹¤í—˜ì„ ìœ„í•œ ê· í˜• ì¡íŒ ì •ê·œí™”"""
    ENV_SCALE = 2.661
    HUMAN_SCALE = 0.357
    
    env_normalized = np.tanh(env_reward / ENV_SCALE)
    human_normalized = np.tanh(human_reward / HUMAN_SCALE)
    
    return env_normalized, human_normalized

# === RLHF ë³´ìƒ ëª¨ë¸ ë¡œë” ===
class HumanRewardModelLoader:
    """ì¸ê°„ í”¼ë“œë°± ë³´ìƒ ëª¨ë¸ ë¡œë”"""
    
    def __init__(self, model_path):
        self.model_path = model_path
        self.model = None
        self.scalers = {}  # ê°œì„ ëœ ëª¨ë¸ìš©
        self.scaler_mean = None  # ê¸°ì¡´ ëª¨ë¸ìš©
        self.scaler_scale = None  # ê¸°ì¡´ ëª¨ë¸ìš©
        self.device = 'cpu'
        self.is_improved_model = False  # ëª¨ë¸ íƒ€ì… êµ¬ë¶„
        
    def load_model(self):
        """ë³´ìƒ ëª¨ë¸ ë¡œë“œ"""
        try:
            if not os.path.exists(self.model_path):
                log_error(f"Reward model file not found: {self.model_path}")
                return False
            
            checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
            log_info(f"Loading reward model: {os.path.basename(self.model_path)}")
            
            # ê°œì„ ëœ ëª¨ë¸ì¸ì§€ í™•ì¸
            if 'scalers' in checkpoint:
                # ê°œì„ ëœ ëª¨ë¸ ë¡œë“œ
                self.is_improved_model = True
                log_info("Loading improved reward model with multiple scalers")
                
                # ìŠ¤ì¼€ì¼ëŸ¬ ì •ë³´ ë³µì›
                from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
                
                scaler_info = checkpoint['scalers']
                for feature, info in scaler_info.items():
                    if info['type'] == 'StandardScaler':
                        scaler = StandardScaler()
                        scaler.mean_ = np.array(info['mean'])
                        scaler.scale_ = np.array(info['scale'])
                    elif info['type'] == 'MinMaxScaler':
                        scaler = MinMaxScaler()
                        scaler.min_ = np.array(info['min'])
                        scaler.scale_ = np.array(info['scale'])
                    elif info['type'] == 'RobustScaler':
                        scaler = RobustScaler()
                        scaler.center_ = np.array(info['center'])
                        scaler.scale_ = np.array(info['scale'])
                    
                    self.scalers[feature] = scaler
                
                # ëª¨ë¸ ìƒì„±
                from architectural_reward_model import SimplifiedRewardModel
                config = checkpoint.get('config', {})
                self.model = SimplifiedRewardModel(
                    state_dim=4,
                    hidden_dim=config.get('hidden_dim', 96)
                )
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.model.eval()
                
                # í•™ìŠµ ì •ë³´ ì¶œë ¥
                if 'best_model_info' in checkpoint and checkpoint['best_model_info']:
                    best_info = checkpoint['best_model_info']
                    log_info(f"âœ… Best model info:")
                    log_info(f"   Validation accuracy: {best_info['val_accuracy']:.3f}")
                    log_info(f"   Reward mean: {best_info['reward_stats']['mean']:.4f}")
                    log_info(f"   Negative ratio: {best_info['reward_stats']['negative_ratio']:.3f}")
                
            else:
                # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
                self.is_improved_model = False
                model_type = checkpoint.get('model_type', 'unknown')
                timestamp = checkpoint.get('timestamp', 'unknown')
                log_info(f"Model type: {model_type}, Timestamp: {timestamp}")
                
                if 'scaler_mean' in checkpoint and 'scaler_scale' in checkpoint:
                    self.scaler_mean = np.array(checkpoint['scaler_mean'])
                    self.scaler_scale = np.array(checkpoint['scaler_scale'])
                    log_success("Scaler information loaded")
                else:
                    log_warning("No scaler information found in checkpoint")
                
                if HUMAN_REWARD_MODEL_AVAILABLE:
                    state_dim = checkpoint.get('state_dim', 4)
                    self.model = ProbabilisticArchitecturalModel_V2(state_dim=state_dim)
                    self.model.load_state_dict(checkpoint['model_state_dict'])
                    self.model.eval()
                else:
                    log_error("ProbabilisticArchitecturalModel_V2 not available")
                    return False
            
            log_success("Human reward model loaded successfully")
            return True
                
        except Exception as e:
            log_error(f"Failed to load reward model: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def predict_reward(self, state_vector):
        """ë³´ìƒ ì˜ˆì¸¡"""
        if self.model is None:
            return 0.0
        
        try:
            if len(state_vector) != 4:
                log_warning(f"Invalid state vector length: {len(state_vector)}")
                return 0.0
            
            # ì œì•½ ì¡°ê±´ ì²´í¬ ì œê±° - í™˜ê²½ ë³´ìƒì´ ì´ë¯¸ ì²˜ë¦¬í•˜ë¯€ë¡œ ì´ì¤‘ ì²˜ë²Œ ë°©ì§€
            bcr, far, winter_sunlight, sv_ratio = state_vector
            
            # ì£¼ì„ ì²˜ë¦¬ ë˜ëŠ” ì œê±°
            # if bcr > 0.7:  # BCR 70% ì´ˆê³¼
            #     return -30.0
            # if far < 2.0:  # FAR 200% ë¯¸ë§Œ
            #     return -30.0
            # if far > 5.0:  # FAR 500% ì´ˆê³¼
            #     return -30.0
            
            # ì„ íƒì : ê·¹ë‹¨ê°’ì— ëŒ€í•œ ì†Œí”„íŠ¸ íŒ¨ë„í‹° (í•„ìš”ì‹œ)
            soft_penalty = 0.0
            if far > 5.0:
                # ì´ˆê³¼ ì •ë„ì— ë¹„ë¡€í•œ ë¶€ë“œëŸ¬ìš´ íŒ¨ë„í‹°
                excess_ratio = min((far - 5.0) / 2.0, 1.0)  # ìµœëŒ€ 1.0
                soft_penalty = -2.0 * excess_ratio  # ìµœëŒ€ -2.0
            elif far < 2.0:
                # ë¯¸ë‹¬ ì •ë„ì— ë¹„ë¡€í•œ ë¶€ë“œëŸ¬ìš´ íŒ¨ë„í‹°
                deficit_ratio = min((2.0 - far) / 2.0, 1.0)
                soft_penalty = -2.0 * deficit_ratio
            
            if bcr > 0.7:
                excess_ratio = min((bcr - 0.7) / 0.3, 1.0)
                soft_penalty += -1.0 * excess_ratio  # ì¶”ê°€ ìµœëŒ€ -1.0
            
            # ì •ìƒ ë²”ìœ„ì¸ ê²½ìš°ë§Œ ëª¨ë¸ ì˜ˆì¸¡ ì§„í–‰
            # ìŠ¤ì¼€ì¼ë§ ì ìš©
            if self.is_improved_model and self.scalers:  # ê°œì„ ëœ ëª¨ë¸
                state_scaled = np.zeros_like(state_vector)
                feature_names = ['BCR', 'FAR', 'WinterTime', 'SVR']
                for i, feature in enumerate(feature_names):
                    if feature in self.scalers:
                        state_scaled[i] = self.scalers[feature].transform(
                            np.array(state_vector[i]).reshape(-1, 1)
                        )[0, 0]
                    else:
                        state_scaled[i] = state_vector[i]
            elif self.scaler_mean is not None and self.scaler_scale is not None:  # ê¸°ì¡´ ëª¨ë¸
                state_scaled = (np.array(state_vector) - self.scaler_mean) / self.scaler_scale
            else:
                state_scaled = np.array(state_vector)
            
            # ëª¨ë¸ ì˜ˆì¸¡
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state_scaled).unsqueeze(0)
                raw_reward = self.model(state_tensor).item()
                
                # ìŠ¤ì¼€ì¼ë§ ì ìš©
                if self.is_improved_model:  # ê°œì„ ëœ ëª¨ë¸
                    # ë²•ì  ì œì•½ ëª…ì‹œì  ì²˜ë¦¬
                    if bcr > 0.7 or far > 5.0 or far < 2.0:
                        # ìœ„ë°˜ ì‹œ ë¬´ì¡°ê±´ ìŒìˆ˜
                        base_penalty = -5.0
                        
                        # ëª¨ë¸ ì˜ˆì¸¡ë„ ê³ ë ¤í•˜ë˜ ì œí•œì ìœ¼ë¡œ
                        model_contribution = raw_reward * 0.5
                        
                        final_reward = base_penalty + np.clip(model_contribution, -5, 2)
                    else:
                        # ì •ìƒ ë²”ìœ„ì—ì„œë§Œ ëª¨ë¸ ì‹ ë¢°
                        final_reward = raw_reward * 2.5
                        final_reward = np.clip(final_reward, -10.0, 8.0)
                else:  # ê¸°ì¡´ ëª¨ë¸
                    final_reward = 8.0 * torch.tanh(torch.tensor(raw_reward / 2.0)).item()
                    final_reward = np.clip(final_reward, -10.0, 10.0)
            
            return final_reward
            
        except Exception as e:
            log_warning(f"Human reward prediction error: {e}")
            return 0.0

# === RLHF ìƒíƒœ ìˆ˜ì‹ ê¸° (ê¸°ì¡´ StateReceiver êµ¬ì¡° ìœ ì§€) ===
class RLHFStateReceiver:
    """RLHFë¥¼ ìœ„í•œ í–¥ìƒëœ ìƒíƒœ ìˆ˜ì‹ ê¸°"""
    
    def __init__(self, port=5557, save_dir=None, human_reward_model=None):
        self.port = port
        self.save_dir = save_dir or ZMQ_LOGS_DIR
        self.human_reward_model = human_reward_model
                
        # í™˜ê²½ ë³´ìƒ í•¨ìˆ˜ë¥¼ ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ìƒì„±
        self.env_reward_function = None
        if REWARD_ADAPTER_AVAILABLE:
            self.env_reward_function = create_reward_function(
                reward_type="optimized",
                bcr_limit=70.0,
                far_min=200.0,
                far_max=500.0,
                use_seasonal=True,
                debug=DEBUG
            )
        
        # ê¸°ì¡´ê³¼ ë™ì¼í•œ êµ¬ì¡°
        self.context = None
        self.socket = None
        self.file = None
        self.csv_file = None
        self.csv_writer = None
        self.running = False
        self.thread = None
        self.stop_event = STOP_EVENT
        self.message_count = 0
        self.data_message_count = 0
        self.health_check_count = 0
        self.start_time = None
        
        # í†µê³„
        self.closed_brep_count = 0
        self.invalid_geometry_count = 0
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ê°œì„ 
        self.metrics_dir = os.path.join(self.save_dir, 'metrics')
        os.makedirs(self.metrics_dir, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ íŒŒì¼ëª…ì´ ì•„ë‹Œ í´ë”ëª…ì—ë§Œ ì‚¬ìš©
        self.log_file_path = os.path.join(self.metrics_dir, "state_reward_log.json")
        self.metrics_file_path = os.path.join(self.metrics_dir, "architecture_metrics.csv")
    
    def initialize(self):
        """ì´ˆê¸°í™”"""
        try:
            # ZMQ ì´ˆê¸°í™”
            self.context = zmq.Context()
            self.socket = self.context.socket(zmq.PULL)
            self.socket.set_hwm(1000)
            bind_address = f"tcp://*:{self.port}"
            self.socket.bind(bind_address)
            log_success(f"ZMQ PULL ì†Œì¼“ì´ {bind_address}ì— ë°”ì¸ë”©ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™”
            self.file = open(self.log_file_path, 'w', encoding='utf-8')
            self.file.write('[\n')
            
            # CSV ë©”íŠ¸ë¦­ íŒŒì¼ ì´ˆê¸°í™”
            self.csv_file = open(self.metrics_file_path, 'w', encoding='utf-8')
            header = "timestamp,step,is_closed_brep,excluded_from_training,bcr,far,winter_sunlight,sv_ratio,env_reward,human_reward,action1,action2,action3,action4"
            self.csv_file.write(header + "\n")
            self.csv_file.flush()
            
            log_info(f"ğŸ“Š ë©”íŠ¸ë¦­ CSV íŒŒì¼ ìƒì„±ë¨: {self.metrics_file_path}")
            
            self.running = True
            self.start_time = time.time()
            return True
            
        except Exception as e:
            log_error(f"ìƒíƒœ ìˆ˜ì‹ ê¸° ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.cleanup()
            return False
    
    def start(self):
        """ìƒíƒœ ìˆ˜ì‹  ì‹œì‘"""
        log_info(f"ğŸ”„ ìƒíƒœ ìˆ˜ì‹ ê¸°ê°€ í¬íŠ¸ {self.port}ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
        log_info(f"ğŸ“ ë°ì´í„°ëŠ” {self.log_file_path}ì— ì €ì¥ë©ë‹ˆë‹¤.")
        log_info("\nğŸ‘‚ ìƒíƒœ ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
        
        self.receive_loop()
    
    def receive_loop(self):
        """ë©”ì‹œì§€ ìˆ˜ì‹  ë£¨í”„"""
        while not self.stop_event.is_set():
            try:
                try:
                    message = self.socket.recv_string(flags=zmq.NOBLOCK)
                    self.process_message(message)
                except zmq.Again:
                    time.sleep(0.1)
                    
                    # íƒ€ì„ì•„ì›ƒ ì²´í¬
                    elapsed = time.time() - self.start_time
                    if self.data_message_count == 0 and elapsed > 60:
                        log_warning(f"\nâ±ï¸ 60ì´ˆ ë™ì•ˆ ìƒíƒœ/ë³´ìƒ ë°ì´í„°ê°€ ì—†ì–´ ìë™ ì¢…ë£Œí•©ë‹ˆë‹¤\n")
                        self.stop_event.set()
                        break
                        
            except Exception as e:
                log_error(f"ë©”ì‹œì§€ ìˆ˜ì‹  ì¤‘ ì˜¤ë¥˜: {e}")
                time.sleep(0.5)
        
        self.cleanup()
    
    def process_message(self, message):
        """ë©”ì‹œì§€ ì²˜ë¦¬ - ê¸°ì¡´ RLê³¼ ë™ì¼í•œ ë¡œì§"""
        try:
            data = json.loads(message)
            self.message_count += 1
            
            log_debug(f"ìˆ˜ì‹ ëœ ë©”ì‹œì§€: {message[:100]}...")
            
            # health_check ë©”ì‹œì§€ í™•ì¸
            is_health_check = data.get("type") == "health_check"
            
            if is_health_check:
                self.health_check_count += 1
                return
            
            # ì‹¤ì œ ë°ì´í„° ë©”ì‹œì§€ ì²˜ë¦¬
            self.data_message_count += 1
            
            # ìƒíƒœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            if 'state' in data:
                # JSON íŒŒì¼ì— ë°ì´í„° ê¸°ë¡
                if self.data_message_count > 1:
                    self.file.write(',\n')
                self.file.write(json.dumps(data, ensure_ascii=False))
                
                # CSV íŒŒì¼ì— ë©”íŠ¸ë¦­ ê¸°ë¡
                if self.csv_file:
                    timestamp = data.get('timestamp', int(time.time() * 1000))
                    
                    # ìƒíƒœ ì²˜ë¦¬ - ê¸°ì¡´ RLê³¼ ë™ì¼
                    state = data.get('state', [0, 0, 0, 0])
                    actions = data.get('action', [0, 0, 0, 0])
                    
                    # Closed Brep í™•ì¸
                    is_closed_brep = False
                    bcr = 0
                    far = 0
                    winter_sunlight = 0
                    sv_ratio = 0
                    
                    # ìƒíƒœ í˜•ì‹ ë¶„ì„ ë° ì²˜ë¦¬
                    if isinstance(state, list):
                        # ìˆ«ì ê°’ê³¼ ë¬¸ìì—´ ë¶„ë¦¬
                        numeric_values = [item for item in state if isinstance(item, (int, float))]
                        string_values = [item for item in state if isinstance(item, str)]
                        
                        log_debug(f"ìƒíƒœ ìˆ«ìê°’: {numeric_values}")
                        if string_values:
                            log_debug(f"ìƒíƒœ ë¬¸ìì—´: {string_values}")
                        
                        # Closed Brep ë¬¸ìì—´ í™•ì¸
                        is_closed_brep = any("Closed Brep" in s for s in string_values) and not any(s == "0" for s in string_values)
                        
                        # ìˆ«ì ê°’ ì²˜ë¦¬
                        if len(numeric_values) >= 4:
                            bcr = numeric_values[0]
                            far = numeric_values[1]
                            winter_sunlight = numeric_values[2]
                            sv_ratio = numeric_values[3]
                            is_closed_brep = True
                        elif len(numeric_values) == 3:
                            bcr = numeric_values[0]
                            far = numeric_values[1]
                            winter_sunlight = numeric_values[2]
                            sv_ratio = 1.0
                        
                        # ì²« ë²ˆì§¸ ìš”ì†Œê°€ "0"ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                        if len(state) > 0 and isinstance(state[0], str) and state[0] == "0":
                            is_closed_brep = False
                            bcr = far = winter_sunlight = sv_ratio = 0
                    
                    # ë³´ìƒ ê³„ì‚°
                    try:
                        if is_closed_brep:
                            # í™˜ê²½ ë³´ìƒ ê³„ì‚°
                            if self.env_reward_function:  # ì´ë¯¸ ìƒì„±ëœ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
                                state_4d = [bcr, far, winter_sunlight, sv_ratio]
                                env_reward, env_info = self.env_reward_function.calculate_reward(state_4d)
                            else:
                                env_reward = 1.0
                                env_info = {}
                            
                            # ì¸ê°„ í”¼ë“œë°± ë³´ìƒ ê³„ì‚°
                            human_reward = 0.0
                            if self.human_reward_model:
                                human_reward = self.human_reward_model.predict_reward([bcr, far, winter_sunlight, sv_ratio])
                            
                            self.closed_brep_count += 1
                        else:
                            # ë¹„ì •ìƒ ìƒíƒœ
                            env_reward = -30.0
                            human_reward = -30.0 # í™˜ê²½ ë³´ìƒê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •
                            env_info = {"error": "Invalid geometry (Not a Closed Brep)"}
                            self.invalid_geometry_count += 1
                        
                        # ë°ì´í„°ì— ë³´ìƒ ì •ë³´ ì¶”ê°€
                        data['env_reward'] = env_reward
                        data['human_reward'] = human_reward
                        data['env_reward_info'] = env_info
                        data['is_closed_brep'] = is_closed_brep
                        
                    except Exception as e:
                        log_error(f"ë³´ìƒ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
                        env_reward = -10.0
                        human_reward = 0.0
                        data['env_reward'] = env_reward
                        data['human_reward'] = human_reward
                        data['reward_error'] = str(e)
                    
                    # CSV í—¤ë” ì‘ì„± (ì²« ë²ˆì§¸ ë©”ì‹œì§€ì¼ ë•Œ)
                    # if self.data_message_count == 1:
                    #     header = "timestamp,step,is_closed_brep,excluded_from_training,bcr,far,winter_sunlight,sv_ratio,env_reward,human_reward"
                    #     for i in range(len(actions[:4])):
                    #         header += f",action{i+1}"
                    #     self.csv_file.write(header + "\n")
                    
                    # CSV ë¼ì¸ ì‘ì„±
                    excluded = 1 if not is_closed_brep else 0
                    csv_line = f"{timestamp},{self.data_message_count},{int(is_closed_brep)},{excluded},{bcr},{far},{winter_sunlight},{sv_ratio},{env_reward},{human_reward}"
                    
                    for action in actions[:4]:
                        csv_line += f",{action}"
                    
                    self.csv_file.write(csv_line + "\n")
                    self.csv_file.flush()
                    
                    # ìƒíƒœë¥¼ íì— ì¶”ê°€
                    formatted_state = [bcr, far, winter_sunlight, sv_ratio]
                    
                    log_debug(f"íì— ìƒíƒœ ì¶”ê°€: state={formatted_state}, reward={env_reward}, is_closed_brep={is_closed_brep}")
                    
                    STATE_QUEUE.put((formatted_state, env_reward, data))
                    global LAST_STATE
                    LAST_STATE = (formatted_state, env_reward, data)
            else:
                log_warning(f"'state' í‚¤ê°€ ì—†ëŠ” ë©”ì‹œì§€: {message[:50]}...")
                        
        except json.JSONDecodeError:
            log_error(f"ì˜ëª»ëœ JSON í˜•ì‹: {message[:100]}...")
        except Exception as e:
            log_error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        log_info("\nğŸ§¹ ìƒíƒœ ìˆ˜ì‹ ê¸° ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        self.running = False
        
        if self.file:
            try:
                self.file.write('\n]')
                self.file.close()
                log_info("ğŸ“ ë¡œê·¸ íŒŒì¼ì´ ë‹«í˜”ìŠµë‹ˆë‹¤.")
            except:
                pass
            self.file = None
        
        if self.csv_file:
            try:
                self.csv_file.close()
            except:
                pass
            self.csv_file = None
        
        if self.socket:
            try:
                self.socket.close()
                log_info("ğŸ”Œ ZMQ ì†Œì¼“ì´ ë‹«í˜”ìŠµë‹ˆë‹¤.")
            except:
                pass
            self.socket = None
        
        if self.context:
            try:
                self.context.term()
                log_info("ğŸ”„ ZMQ ì»¨í…ìŠ¤íŠ¸ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            except:
                pass
            self.context = None
        
        # í†µê³„ ì¶œë ¥
        elapsed = time.time() - self.start_time if self.start_time else 0
        log_info(f"\nğŸ“Š ì´ ìˆ˜ì‹ : {self.message_count}ê°œ ë©”ì‹œì§€ (ì‹¤ì œ ë°ì´í„°: {self.data_message_count}ê°œ, health_check: {self.health_check_count}ê°œ)")
        log_info(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {elapsed:.1f}ì´ˆ")
        rate = self.data_message_count / elapsed if elapsed > 0 else 0
        log_info(f"âš¡ í‰ê·  ìˆ˜ì‹  ì†ë„: {rate:.1f}ê°œ/ì´ˆ")
        log_info(f"âœ… Closed Brep ìˆ˜: {self.closed_brep_count}")
        log_info(f"âŒ Invalid geometry ìˆ˜: {self.invalid_geometry_count}")

# === RLHF í†µí•© í™˜ê²½ (ê¸°ì¡´ ArchitectureOptimizationEnv êµ¬ì¡° ìœ ì§€) ===
class RLHFArchitectureOptimizationEnv(gym.Env):
    """RLHFê°€ í†µí•©ëœ ê±´ì¶• ì„¤ê³„ ìµœì í™” í™˜ê²½"""
    
    def __init__(self, 
                 action_port=5556, 
                 state_port=5557,
                 reward_weights={'env': 0.7, 'human': 0.3},
                 human_reward_model_path=None,
                 bcr_limit=70.0,
                 far_min_limit=200.0,
                 far_max_limit=500.0,
                 slider_mins=None,
                 slider_maxs=None,
                 wait_time=5.0,
                 initial_wait=6.0):
        
        super(RLHFArchitectureOptimizationEnv, self).__init__()
        
        # í¬íŠ¸ ì„¤ì •
        self.action_port = action_port
        self.state_port = state_port
        
        # ë³´ìƒ ê°€ì¤‘ì¹˜
        self.reward_weights = reward_weights
        
        # ê±´ì¶• ì œí•œ
        self.bcr_limit = bcr_limit
        self.far_min_limit = far_min_limit
        self.far_max_limit = far_max_limit
        
        # ì‹œê°„ ì„¤ì •
        self.wait_time = wait_time
        self.initial_wait = initial_wait
        
        # ìŠ¬ë¼ì´ë” ë²”ìœ„
        self.slider_mins = np.array([10.0, 50.0, 0.0, 0.0]) if slider_mins is None else np.array(slider_mins)
        self.slider_maxs = np.array([25.0, 100.0, 100.0, 100.0]) if slider_maxs is None else np.array(slider_maxs)
        log_info(f"ğŸ“ ìŠ¬ë¼ì´ë” ì‹¤ì œ ë²”ìœ„: ìµœì†Œê°’={self.slider_mins}, ìµœëŒ€ê°’={self.slider_maxs}")
        
        # ì•¡ì…˜/ìƒíƒœ ê³µê°„
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(4,), dtype=np.float32)
        self.observation_space = spaces.Box(
            low=np.array([0.0, 0.0, 0.0, 0.0]),
            high=np.array([1.0, 10.0, 200000.0, 6.0]),
            dtype=np.float32
        )
        
        # í™˜ê²½ ë³´ìƒ í•¨ìˆ˜
        if REWARD_ADAPTER_AVAILABLE:
            self.env_reward_function = create_reward_function(
                reward_type="optimized",
                bcr_limit=bcr_limit,
                far_min=far_min_limit,
                far_max=far_max_limit,
                use_seasonal=True,
                debug=DEBUG
            )
        else:
            self.env_reward_function = None
        
        # ì¸ê°„ í”¼ë“œë°± ë³´ìƒ ëª¨ë¸
        self.human_reward_model = None
        if human_reward_model_path:
            self.human_reward_model = HumanRewardModelLoader(human_reward_model_path)
            if not self.human_reward_model.load_model():
                self.human_reward_model = None
        
        # ZMQ ì´ˆê¸°í™”
        self.context = None
        self.action_socket = None
        self._initialize_zmq()
        
        # ì—í”¼ì†Œë“œ ì¶”ì 
        self.episode_steps = 0
        self.total_steps = 0
        self.current_state = None
        self.current_env_reward = 0.0
        self.current_human_reward = 0.0
        self.current_info = {}
        
        # í†µê³„ ì¶”ì 
        self.reward_stats = {
            'env_rewards': [],
            'human_rewards': [],
            'combined_rewards': [],
            'closed_brep_count': 0,
            'invalid_geometry_count': 0
        }
        
        log_info(f"ğŸ—ï¸ RLHF ê±´ì¶• ìµœì í™” í™˜ê²½ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        log_info(f"   - BCR ì œí•œ: {self.bcr_limit}%")
        log_info(f"   - FAR í—ˆìš© ë²”ìœ„: {self.far_min_limit}% ~ {self.far_max_limit}%")
        log_info(f"   - ë³´ìƒ ê°€ì¤‘ì¹˜: í™˜ê²½={self.reward_weights['env']}, ì¸ê°„={self.reward_weights['human']}")
        log_info(f"   - ì•¡ì…˜ ê³µê°„: {self.action_space}")
        log_info(f"   - ìƒíƒœ ê³µê°„: {self.observation_space}")
    
    def _initialize_zmq(self):
        """ZMQ í†µì‹  ì´ˆê¸°í™”"""
        try:
            self.context = zmq.Context()
            self.action_socket = self.context.socket(zmq.PUSH)
            self.action_socket.set_hwm(1000)
            self.action_socket.setsockopt(zmq.LINGER, 500)
            bind_address = f"tcp://*:{self.action_port}"
            self.action_socket.bind(bind_address)
            log_success(f"ì•¡ì…˜ ì „ì†¡ ZMQ PUSH ì†Œì¼“ì´ {bind_address}ì— ë°”ì¸ë”©ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
        except Exception as e:
            log_error(f"ZMQ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            return False
    
    def _normalize_actions(self, actions):
        """ì•¡ì…˜ ì •ê·œí™”"""
        actions_0_1 = (actions + 1.0) / 2.0
        real_actions = self.slider_mins + actions_0_1 * (self.slider_maxs - self.slider_mins)
        return real_actions
    
    def _send_action(self, action_values):
        """ZMQë¥¼ í†µí•´ ì•¡ì…˜ ì „ì†¡"""
        try:
            action_list = action_values.tolist()
            action_json = json.dumps(action_list)
            self.action_socket.send_string(action_json)
            log_debug(f"ì•¡ì…˜ ì „ì†¡ë¨: {action_json}")
            return True
        except Exception as e:
            log_error(f"ì•¡ì…˜ ì „ì†¡ ì˜¤ë¥˜: {e}")
            return False
    
    def _wait_for_state(self, timeout=20.0):
        """ìƒíƒœ ëŒ€ê¸°"""
        start_time = time.time()
        log_info("ğŸ‘‚ ìƒˆ ìƒíƒœ ë°ì´í„° ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
        log_info(f"â±ï¸ ìƒˆ ìƒíƒœ ì´ë²¤íŠ¸ ëŒ€ê¸° ì‹œì‘ (íƒ€ì„ì•„ì›ƒ: {int(timeout)}ì´ˆ)")
        
        while not STOP_EVENT.is_set():
            elapsed = time.time() - start_time
            
            if elapsed > timeout:
                log_info(f"â±ï¸ ìƒˆ ìƒíƒœ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ ({elapsed:.1f}ì´ˆ), ìµœì‹  ì‚¬ìš© ê°€ëŠ¥ ìƒíƒœ ë°˜í™˜")
                return False
            
            if int(elapsed) % 3 == 0 and elapsed % 3 < 0.1:
                log_info(f"â±ï¸ ìƒíƒœ ëŒ€ê¸° ì¤‘... ({elapsed:.1f}/{timeout}ì´ˆ)")
            
            try:
                state, reward, info = STATE_QUEUE.get(block=False)
                
                if len(state) == 4:
                    self.current_state = np.array(state, dtype=np.float32)
                else:
                    log_error(f"âŒ ìƒíƒœ í˜•ì‹ ì˜¤ë¥˜: {state}")
                    continue
                
                # RLHF: infoì—ì„œ ë¶„ë¦¬ëœ ë³´ìƒ ì •ë³´ ì¶”ì¶œ
                self.current_env_reward = info.get('env_reward', reward)
                self.current_human_reward = info.get('human_reward', 0.0)
                self.current_info = info
                return True
                
            except queue.Empty:
                time.sleep(0.1)
                continue
        
        return False

    def _get_last_state(self):
        """ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì‹ ëœ ìƒíƒœë¥¼ ë°˜í™˜"""
        global LAST_STATE
        if LAST_STATE is not None:
            state, reward, info = LAST_STATE
            if len(state) == 4:
                env_reward = info.get('env_reward', reward)
                human_reward = info.get('human_reward', 0.0)
                return np.array(state, dtype=np.float32), env_reward, human_reward, info
        
        return np.zeros(4, dtype=np.float32), 0.0, 0.0, {}
    
    def _clear_state_queue(self):
        """ìƒíƒœ í ë¹„ìš°ê¸°"""
        log_debug("ì´ì „ ìƒíƒœ í ë¹„ìš°ëŠ” ì¤‘...")
        count = 0
        while True:
            try:
                STATE_QUEUE.get(block=False)
                count += 1
            except queue.Empty:
                break
        
        if count > 0:
            log_debug(f"{count}ê°œì˜ ì´ì „ ìƒíƒœ í•­ëª©ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤.")
    
    def reset(self, seed=None, options=None):
        """í™˜ê²½ ì´ˆê¸°í™”"""
        super().reset(seed=seed)
        self.episode_steps = 0
        
        # í™˜ê²½ ë³´ìƒ í•¨ìˆ˜ ì´ì „ ìƒíƒœ ì´ˆê¸°í™”
        if self.env_reward_function and hasattr(self.env_reward_function, 'reset_prev_state'):
            self.env_reward_function.reset_prev_state()
        
        # ì´ˆê¸° ì•¡ì…˜ ìƒì„±
        initial_action = np.zeros(4, dtype=np.float32)
        
        real_action = self._normalize_actions(initial_action)
        log_info(f"ğŸ”„ í™˜ê²½ ì´ˆê¸°í™”: ì •ê·œí™”ëœ ì´ˆê¸° ì•¡ì…˜={initial_action}, ì‹¤ì œ ê°’={real_action}")
        
        self._send_action(real_action)
        
        log_info(f"â±ï¸ ì´ˆê¸°í™” ì¤‘ {self.initial_wait}ì´ˆ ëŒ€ê¸°...")
        time.sleep(self.initial_wait)
        
        log_info("ğŸ‘‚ ì´ˆê¸° ìƒíƒœ ë°ì´í„° ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
        if self._wait_for_state(timeout=30.0):
            initial_state = self.current_state
            initial_info = self.current_info
        else:
            log_warning("ì´ˆê¸° ìƒíƒœë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©.")
            initial_state, _, _, initial_info = self._get_last_state()
        
        log_info(f"ì´ˆê¸° ìƒíƒœ: {initial_state}")
        
        return initial_state, initial_info
    
    def step(self, action):
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰"""
        log_debug(f"ì›ë³¸ ì•¡ì…˜: {action}")
        
        # ì•¡ì…˜ ë²”ìœ„ ì²´í¬
        if np.any(np.abs(action) > 1.0):
            log_warning(f"ì•¡ì…˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨: {action}")
            action = np.clip(action, -1.0, 1.0)
        
        log_info(f"\nğŸ® ìŠ¤í… {self.total_steps}, ì •ê·œí™”ëœ ì•¡ì…˜: {action}")
        
        # ì‹¤ì œ ìŠ¬ë¼ì´ë” ê°’ìœ¼ë¡œ ë³€í™˜
        real_action = self._normalize_actions(action)
        log_info(f"ğŸ“Š ì‹¤ì œ ìŠ¬ë¼ì´ë” ê°’: {real_action}")
        
        # ì´ì „ ìƒíƒœ í ë¹„ìš°ê¸°
        self._clear_state_queue()
        
        # ì•¡ì…˜ ì „ì†¡
        self._send_action(real_action)
        
        # ì²˜ë¦¬ ëŒ€ê¸°
        log_info(f"â±ï¸ Grasshopper ì²˜ë¦¬ë¥¼ ìœ„í•´ {self.wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...")
        time.sleep(self.wait_time)
        
        # ì¬ì‹œë„ ë¡œì§ (Closed Brep í™•ì¸)
        max_retries = 5
        retries = 0
        valid_state_received = False
        
        while retries < max_retries and not valid_state_received:
            if self._wait_for_state():
                state = self.current_state
                env_reward = self.current_env_reward
                human_reward = self.current_human_reward
                info = self.current_info
                
                # Closed Brep í™•ì¸
                is_valid_state = info.get('is_closed_brep', True)
                
                if is_valid_state:
                    valid_state_received = True
                    self.reward_stats['closed_brep_count'] += 1
                else:
                    self.reward_stats['invalid_geometry_count'] += 1
                    retries += 1
                    if retries < max_retries:
                        # ì¬ì‹œë„ ì „ëµ
                        if retries == 1:
                            noise = np.random.normal(0, 0.1, size=action.shape)
                            new_action = np.clip(action + noise, -1.0, 1.0)
                        elif retries == 2:
                            new_action = np.clip(action * 0.95, -1.0, 1.0)
                        elif retries == 3:
                            new_action = np.clip(action * 1.05, -1.0, 1.0)
                        else:
                            noise = np.random.normal(0, 0.3, size=action.shape)
                            new_action = np.clip(action + noise, -1.0, 1.0)
                        
                        log_info(f"ğŸ”„ ìœ íš¨í•˜ì§€ ì•Šì€ ìƒíƒœë¡œ ì¸í•œ ì¬ì‹œë„ {retries}/{max_retries}, ìˆ˜ì •ëœ ì•¡ì…˜: {new_action}")
                        
                        real_action = self._normalize_actions(new_action)
                        self._send_action(real_action)
                        time.sleep(self.wait_time)
            else:
                log_warning("Grasshopperì—ì„œ ìƒíƒœë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ì „ ìƒíƒœ ì‚¬ìš©.")
                state, env_reward, human_reward, info = self._get_last_state()
                valid_state_received = True
        
        # ëª¨ë“  ì¬ì‹œë„ í›„ì—ë„ ìœ íš¨í•œ ìƒíƒœë¥¼ ë°›ì§€ ëª»í•œ ê²½ìš°
        if not valid_state_received:
            log_warning(f"âš ï¸ {max_retries}ë²ˆì˜ ì‹œë„ í›„ì—ë„ ìœ íš¨í•œ ìƒíƒœë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            truncated = True
            env_reward = -30.0
            human_reward = -10.0
            state, _, _, info = self._get_last_state()
            info['excluded_from_training'] = True
            info['reason'] = "Invalid geometry (Not a Closed Brep) after max retries"
            terminated = False
        else:
            truncated = False
            terminated = False
        
        # === RLHF ìµœì¢… ë³´ìƒ ê³„ì‚° ===
        print(f"ğŸš¨ğŸš¨ğŸš¨ DEBUG: ì •ê·œí™” ì ìš© ì „ - env: {env_reward:.4f}, human: {human_reward:.4f}")
        env_norm, human_norm = normalize_rewards_for_rlhf(env_reward, human_reward)
        print(f"ğŸš¨ğŸš¨ğŸš¨ DEBUG: ì •ê·œí™” ì ìš© í›„ - env_norm: {env_norm:.4f}, human_norm: {human_norm:.4f}")
        combined_reward = (
            self.reward_weights['env'] * env_norm +
            self.reward_weights['human'] * human_norm
        ) * 5.0
        print(f"ğŸš¨ğŸš¨ğŸš¨ DEBUG: ìµœì¢… ê²°í•© ë³´ìƒ: {combined_reward:.4f}")
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.reward_stats['env_rewards'].append(env_reward)
        self.reward_stats['human_rewards'].append(human_reward)
        self.reward_stats['combined_rewards'].append(combined_reward)
        
        # ìƒíƒœì™€ ë³´ìƒ ë¡œê¹…
        bcr = state[0] * 100.0 if len(state) > 0 else 0.0
        far = state[1] * 100.0 if len(state) > 1 else 0.0
        
        if len(state) >= 4:
            winter_sunlight = state[2]
            sv_ratio = state[3]
            log_info(f"ğŸ“Š BCR: {bcr:.1f}%, FAR: {far:.1f}%, ê²¨ìš¸ ì¼ì‚¬ëŸ‰: {winter_sunlight:.2f}, í‘œë©´ì  ì²´ì ë¹„: {sv_ratio:.4f}")
        
        is_closed_brep = info.get('is_closed_brep', True)
        log_info(f"ğŸ’° ë³´ìƒ: í™˜ê²½={env_reward:.4f}, ì¸ê°„={human_reward:.4f}, ê²°í•©={combined_reward:.4f} (Closed Brep: {'Yes' if is_closed_brep else 'No'})")
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´
        if not truncated:
            truncated = self.episode_steps >= 50
        
        # ì •ë³´ ì—…ë°ì´íŠ¸
        info.update({
            'episode_steps': self.episode_steps,
            'total_steps': self.total_steps,
            'actual_action': real_action.tolist(),
            'is_closed_brep': is_closed_brep,
            'env_reward': env_reward,
            'human_reward': human_reward,
            'combined_reward': combined_reward,
            'reward_weights': self.reward_weights
        })
        
        self.episode_steps += 1
        self.total_steps += 1
        
        return state, combined_reward, terminated, truncated, info
    
    def get_reward_statistics(self):
        """ë³´ìƒ í†µê³„ ë°˜í™˜"""
        if not self.reward_stats['combined_rewards']:
            return {}
        
        return {
            'env_reward_mean': np.mean(self.reward_stats['env_rewards']),
            'human_reward_mean': np.mean(self.reward_stats['human_rewards']),
            'combined_reward_mean': np.mean(self.reward_stats['combined_rewards']),
            'env_reward_std': np.std(self.reward_stats['env_rewards']),
            'human_reward_std': np.std(self.reward_stats['human_rewards']),
            'combined_reward_std': np.std(self.reward_stats['combined_rewards']),
            'total_steps': len(self.reward_stats['combined_rewards']),
            'closed_brep_count': self.reward_stats['closed_brep_count'],
            'invalid_geometry_count': self.reward_stats['invalid_geometry_count'],
            'closed_brep_ratio': self.reward_stats['closed_brep_count'] / (self.reward_stats['closed_brep_count'] + self.reward_stats['invalid_geometry_count']) if (self.reward_stats['closed_brep_count'] + self.reward_stats['invalid_geometry_count']) > 0 else 0
        }
    
    def close(self):
        """í™˜ê²½ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        log_info("ğŸ§¹ í™˜ê²½ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        if self.action_socket:
            try:
                self.action_socket.close()
                log_info("ğŸ”Œ ì•¡ì…˜ ZMQ ì†Œì¼“ì´ ë‹«í˜”ìŠµë‹ˆë‹¤.")
            except:
                pass
            self.action_socket = None
        
        if self.context:
            try:
                self.context.term()
                log_info("ğŸ”„ ì•¡ì…˜ ZMQ ì»¨í…ìŠ¤íŠ¸ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            except:
                pass
            self.context = None
        
        super().close()

# === PPO í•™ìŠµ í•¨ìˆ˜ (ê¸°ì¡´ RLê³¼ ë™ì¼) ===
def train_rlhf_ppo(env, 
                    total_timesteps=10000, 
                    learning_rate=0.0003, 
                    save_dir=None, 
                    log_dir=None,
                    initial_model_path=None):
    """RLHF PPO í•™ìŠµ"""
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    if save_dir is None:
        save_dir = os.path.join(DATA_DIR, "models")
    os.makedirs(save_dir, exist_ok=True)
    
    if log_dir is None:
        log_dir = os.path.join(DATA_DIR, "logs")
    os.makedirs(log_dir, exist_ok=True)
    
    # ê¸°ì¡´ ì½”ë“œ ì œê±°:
    # timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    # model_name = f"rlhf_ppo_architecture_{timestamp}"
    # model_path = os.path.join(save_dir, model_name)
    
    # ìƒˆë¡œìš´ ì½”ë“œ (ëª¨ë¸ ê²½ë¡œ ë‹¨ìˆœí™”):
    model_path = save_dir  # ë°”ë¡œ save_dir ì‚¬ìš©
    
    # ì²´í¬í¬ì¸íŠ¸ ì½œë°± ì„¤ì •
    checkpoint_callback = CheckpointCallback(
        save_freq=1000,
        save_path=model_path,
        name_prefix="checkpoint"
    )
    
    # ëª¨ë¸ ì„¤ì •
    if initial_model_path and os.path.exists(initial_model_path):
        log_info(f"ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì¤‘: {initial_model_path}")
        model = PPO.load(initial_model_path, env=env)
    else:
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=3e-4,
            n_steps=512,
            batch_size=64,
            n_epochs=5,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            normalize_advantage=True,
            ent_coef=0.01,
            vf_coef=0.5,
            max_grad_norm=0.5,
            tensorboard_log=log_dir,
            policy_kwargs=dict(
                net_arch=[dict(pi=[64, 64], vf=[64, 64])],
                activation_fn=torch.nn.Tanh
            ),
            verbose=1
        )
    
    # ì´ ìŠ¤í… ìˆ˜ì— ëŒ€í•œ ì •ë³´ ì¶œë ¥
    log_info(f"ğŸ”„ ì´ {total_timesteps}ê°œì˜ íƒ€ì„ìŠ¤í… ë™ì•ˆ í•™ìŠµí•©ë‹ˆë‹¤...")
    
    try:
        # í•™ìŠµ ì‹œì‘
        model.learn(
            total_timesteps=total_timesteps,
            callback=checkpoint_callback,
            progress_bar=True
        )
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_model_path = os.path.join(model_path, "final_model")
        model.save(final_model_path)
        log_success(f"ğŸ‰ í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ {final_model_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return model, final_model_path
    except KeyboardInterrupt:
        log_info("\nâ¹ï¸ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # ì¤‘ë‹¨ëœ ëª¨ë¸ ì €ì¥
        interrupted_model_path = os.path.join(model_path, "interrupted_model")
        model.save(interrupted_model_path)
        log_info(f"ğŸ›‘ ì¤‘ë‹¨ëœ ëª¨ë¸ì´ {interrupted_model_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return model, interrupted_model_path
    except Exception as e:
        log_error(f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

# === ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ===
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="RLHF í†µí•© ê±´ì¶• ì„¤ê³„ ìµœì í™”")
    
    # ëª¨ë¸ ê²½ë¡œ
    parser.add_argument('--reward-model-path', type=str, 
                       default=str(DEFAULT_PATHS['reward_model']),
                       help='ì¸ê°„ í”¼ë“œë°± ë³´ìƒ ëª¨ë¸ ê²½ë¡œ')
    parser.add_argument('--initial-model', type=str, default=None,
                       help='ì—°ì† í•™ìŠµì„ ìœ„í•œ ì´ˆê¸° PPO ëª¨ë¸ ê²½ë¡œ')
    
    # ë³´ìƒ ê°€ì¤‘ì¹˜
    parser.add_argument('--env-weight', type=float, default=0.7,
                       help='í™˜ê²½ ë³´ìƒ ê°€ì¤‘ì¹˜')
    parser.add_argument('--human-weight', type=float, default=0.3,
                       help='ì¸ê°„ í”¼ë“œë°± ë³´ìƒ ê°€ì¤‘ì¹˜')
    
    # í•™ìŠµ ì„¤ì •
    parser.add_argument('--timesteps', type=int, default=3000,
                       help='ì´ í•™ìŠµ íƒ€ì„ìŠ¤í…')
    parser.add_argument('--learning-rate', type=float, default=3e-4,
                       help='í•™ìŠµë¥ ')
    
    # í¬íŠ¸ ì„¤ì •
    parser.add_argument('--action-port', type=int, default=5556,
                       help='ZMQ ì•¡ì…˜ í¬íŠ¸ (Grasshopperë¡œ)')
    parser.add_argument('--state-port', type=int, default=5557,
                       help='ZMQ ìƒíƒœ í¬íŠ¸ (Grasshopperì—ì„œ)')
    
    # ì¶œë ¥ ì„¤ì •
    parser.add_argument('--output-dir', type=str, 
                       default=str(DEFAULT_PATHS['base_output_dir']),
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--debug', action='store_true',
                       help='ë””ë²„ê·¸ ë¡œê¹… í™œì„±í™”')
    
    args = parser.parse_args()
    
    # ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
    global DEBUG
    DEBUG = args.debug
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ë³´ìƒ ê°€ì¤‘ì¹˜ ì„¤ì •
    reward_weights = {'env': args.env_weight, 'human': args.human_weight}
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # í—¤ë” ì¶œë ¥
    print("\n" + "="*80)
    print(f"ğŸ—ï¸  RLHF í†µí•© ê±´ì¶• ì„¤ê³„ ìµœì í™”")
    print("="*80)
    print(f"ğŸ•’ ì„¸ì…˜ ID: {timestamp}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"ğŸ’» í•™ìŠµ ë””ë°”ì´ìŠ¤: {'cuda' if torch.cuda.is_available() else 'cpu'}")
    print(f"âš–ï¸  ë³´ìƒ ê°€ì¤‘ì¹˜: í™˜ê²½={args.env_weight}, ì¸ê°„={args.human_weight}")
    print(f"ğŸ“Š í•™ìŠµ íƒ€ì„ìŠ¤í…: {args.timesteps:,}")
    print(f"ğŸ§  ì¸ê°„ ë³´ìƒ ëª¨ë¸: {os.path.basename(args.reward_model_path)}")
    if args.initial_model:
        print(f"ğŸ“¦ ì´ˆê¸° ëª¨ë¸: {os.path.basename(args.initial_model)}")
    print("")
    
    # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        print("[1/4] StateReceiver ì´ˆê¸°í™” ì¤‘...")
        
        # StateReceiver ì´ˆê¸°í™” ë° ì‹œì‘
        human_reward_model = HumanRewardModelLoader(args.reward_model_path)
        if not human_reward_model.load_model():
            human_reward_model = None
            log_warning("ì¸ê°„ ë³´ìƒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ë”ë¯¸ ë³´ìƒ ì‚¬ìš©")
        
        state_receiver = RLHFStateReceiver(
            port=args.state_port,
            save_dir=output_dir / 'zmq_logs',
            human_reward_model=human_reward_model
        )
        
        if not state_receiver.initialize():
            log_error("StateReceiver ì´ˆê¸°í™” ì‹¤íŒ¨")
            return
        
        # StateReceiver ìŠ¤ë ˆë“œ ì‹œì‘
        receiver_thread = threading.Thread(target=state_receiver.start)
        receiver_thread.daemon = True
        receiver_thread.start()
        
        print("[2/4] RLHF í™˜ê²½ ì´ˆê¸°í™” ì¤‘...")
        
        # RLHF í™˜ê²½ ìƒì„±
        env = RLHFArchitectureOptimizationEnv(
            action_port=args.action_port,
            state_port=args.state_port,
            reward_weights=reward_weights,
            human_reward_model_path=args.reward_model_path,
            wait_time=5.0,
            initial_wait=6.0
        )
        
        print("[3/4] PPO í•™ìŠµ ì‹œì‘...")
        
        # PPO í•™ìŠµ ì‹¤í–‰
        model, model_path = train_rlhf_ppo(
            env=env,
            total_timesteps=args.timesteps,
            learning_rate=args.learning_rate,
            save_dir=output_dir / 'models',
            log_dir=output_dir / 'logs',
            initial_model_path=args.initial_model
        )
        
        if model and not STOP_EVENT.is_set():
            print("[4/4] ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
            
            # í†µê³„ ìˆ˜ì§‘
            env_stats = env.get_reward_statistics()
            
            # ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
            results = {
                'session_info': {
                    'timestamp': timestamp,
                    'reward_weights': reward_weights,
                    'total_timesteps': args.timesteps,
                    'reward_model': args.reward_model_path,
                    'initial_model': args.initial_model,
                    'final_model': model_path
                },
                'training_statistics': env_stats,
                'final_performance': {
                    'env_reward_mean': env_stats.get('env_reward_mean', 0),
                    'human_reward_mean': env_stats.get('human_reward_mean', 0),
                    'combined_reward_mean': env_stats.get('combined_reward_mean', 0),
                    'closed_brep_ratio': env_stats.get('closed_brep_ratio', 0),
                    'total_steps': env_stats.get('total_steps', 0)
                }
            }
            
            # ê²°ê³¼ ì €ì¥
            results_path = output_dir / 'training_results.json'
            with open(results_path, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            log_success(f"ğŸ“Š í•™ìŠµ ê²°ê³¼ê°€ {results_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print(f"\nâœ… í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ê²°ê³¼ê°€ ì €ì¥ëœ ìœ„ì¹˜: {output_dir}")
        
    except Exception as e:
        log_error(f"í•™ìŠµ ì‹¤íŒ¨: {e}")
        raise
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        print("\nğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        # í™˜ê²½ ì •ë¦¬
        try:
            env.close()
        except:
            pass
        
        # StateReceiver ì •ë¦¬
        if 'state_receiver' in locals():
            try:
                STOP_EVENT.set()
                if 'receiver_thread' in locals() and receiver_thread.is_alive():
                    receiver_thread.join(timeout=5.0)
                state_receiver.cleanup()
            except:
                pass
        
        print("ğŸ’¯ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")

if __name__ == "__main__":
    main()